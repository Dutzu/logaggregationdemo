####
## Source descriptions:
##

# TCP listener that catches local docker container logs
<source>
	@type forward
	port 24224
	bind 0.0.0.0
</source>

# Built in monitoring to show status of plugins http://localhost:24220/api/plugins.json
# Sample output:
# {
#   "plugins": [
#     {
#       "plugin_id": "object:13fa9c0",
#       "plugin_category": "input",
#       "type": "monitor_agent",
#       "config": {
#         "@type": "monitor_agent",
#         "bind": "0.0.0.0",
#         "port": "24220",
#         "tag": "test.monitoring"
#       },
#       "output_plugin": false,
#       "retry_count": null
#     },
#     {
#       "plugin_id": "object:12b6938",
#       "plugin_category": "output",
#       "type": "stdout",
#       "config": {
#         "type": "stdout"
#       },
#       "output_plugin": true,
#       "retry_count": null
#     }
#   ]
# }

<source>
	@type monitor_agent
	bind 0.0.0.0
	port 24220
</source>

# Docker metriscs source - ENABLE if needed
#
# Sample DATA
#
# 2016-01-01 16:17:28 +0200 docker.metrics.memory.stat: {"key":"memory_stat_total_active_file","value":143360,"type":"gauge","hostname":"HGROCLTI01019vm","id":"a68ff14d9e121f5c01049026ed7fad3742ec748c33951a16ecae7ce3bdae3649","name":"/clever_davinci"}
# 2016-01-01 16:17:28 +0200 docker.metrics.cpuacct.stat: {"key":"cpuacct_stat_user","value":743,"type":"counter","hostname":"HGROCLTI01019vm","id":"a68ff14d9e121f5c01049026ed7fad3742ec748c33951a16ecae7ce3bdae3649","name":"/clever_davinci"}
# 2016-01-01 16:17:28 +0200 docker.metrics.memory.stat: {"key":"memory_stat_total_unevictable","value":0,"type":"gauge","hostname":"HGROCLTI01019vm","id":"a68ff14d9e121f5c01049026ed7fad3742ec748c33951a16ecae7ce3bdae3649","name":"/clever_davinci"}
# 2016-01-01 16:17:28 +0200 docker.metrics.cpuacct.stat: {"key":"cpuacct_stat_system","value":102,"type":"counter","hostname":"HGROCLTI01019vm","id":"a68ff14d9e121f5c01049026ed7fad3742ec748c33951a16ecae7ce3bdae3649","name":"/clever_davinci"}
<source>
  type docker_metrics
  stats_interval 1s
  tag_prefix docker.metrics
</source>


####
## Filter descriptions:
##

# Add Source project name to all messages for easier filtering in kibana

<filter *>
  @type record_transformer
  enable_ruby true
  <record>
      sourceEnvironment "#{ENV['FLUENTD_ENVIRONMENT']}"
      hostname "#{Socket.gethostbyname(Socket.gethostname).first}"
  </record>
</filter>

#This filter will remove any illegal bytes that are not conform to UTF-8

<filter encoding.**>
	type record_transformer
  enable_ruby true
	<record>
          message ${ message.encode( 'UTF-8', 'binary', :invalid => :replace, :undef => :replace)}
	</record>
</filter>

# Timestamp fix for lines that could be parsed into separate datetime components

# <filter myapps.**>
#   @type record_transformer
#   enable_ruby true
#   <record>
#       @timestamp ${date_string + "T" + time_string + "." + msec_string + "Z"}
#   </record>
#   remove_keys date_string,time_string,msec_string
# </filter>

####
## Output descriptions:
##

# Sample log:
# 2016-01-01 16:52:21.000000000 +0200 docker.0fe0bd8fefdf: {"log":"2016-12-13 14:52:21,770 [main] [||||] INFO  o.apache.catalina.startup.Catalina - Server
# startup in 107088 ms","container_id":"0fe0bd8fefdf824d0191a3674bd1d2731d0e87a58a667b2f2b655ea5e15ad14a","container_name":"/compose_openam_1","source":"
# stdout"}

# Re-tag and re-emit messages to match them to specific parsing rules based on the container name
# Syntax: rewriterule<num> <attribute> <regex_pattern> <new_tag>
# For basic filtering we use a contains regex pattern like: \/wildfly
# <match docker.**>
# 	@type rewrite_tag_filter
#   #be careful, here the name of the container is weakly linked to the name of the docker-compose service
#   #this is so that we can have strict control over what messages we try to parse with more exact rules.
# 	rewriterule1 container_name wildfly myapps.wildfly
# 	rewriterule2 container_name .* myapps.other
# </match>

# <match fs.**>
# 	@type copy
#   ### Enable for debugging purposes, this will output all logs to stdout
#   ### and also send them to the aggregator
# 	<store>
# 		@type stdout
# 	</store>
# 	<store>
#     @type secure_forward

#     shared_key "#{ENV['FLUENTD_SHARED_KEY']}"
#     self_hostname "#{Socket.gethostbyname(Socket.gethostname).first}"

#     secure true
#     ca_cert_path /fluentd/certs/ca_cert.pem
#      <server>
#         host "#{ENV['FLUENTD_AGGREGATOR_HOST']}"
#         port "#{ENV['FLUENTD_AGGREGATOR_PORT']}"
#         username "#{ENV['FLUENTD_AGGREGATOR_USERNAME']}"
#         password "#{ENV['FLUENTD_AGGREGATOR_PASSWORD']}"
#     </server>

#     buffer_type file
#     buffer_path /fluentd/buffer/log.*.buffer
#     buffer_chunk_limit 8m
#     buffer_queue_limit 128
#     flush_interval 10s
#     flush_at_shutdown true
#     retry_wait 30s
#     max_retry_wait 1h
#     disable_retry_limit true
#     keepalive 300
# 	</store>
# </match>


<match docker.metrics.**>
	@type copy
	# <store>
	# 	@type stdout
	# </store>
	<store>
        @type elasticsearch

        logstash_format true
        logstash_prefix metrics
        type_name metrics

        host elasticsearch
        port 9200

        buffer_type file
        buffer_path /fluentd/metrics_buffer/log.*.buffer
        buffer_chunk_limit 8m
        buffer_queue_limit 128
        flush_interval 10s
        flush_at_shutdown true
        retry_wait 30s
        max_retry_wait 1h
        disable_retry_limit true
        keepalive 300
	</store>
</match>

<match docker.**>
	@type copy
	<store>
		@type stdout
	</store>
	<store>
        @type elasticsearch

        logstash_format true
        logstash_prefix myapps
        type_name fluentd

        host elasticsearch
        port 9200

        buffer_type file
        buffer_path /fluentd/buffer/log.*.buffer
        buffer_chunk_limit 8m
        buffer_queue_limit 128
        flush_interval 10s
        flush_at_shutdown true
        retry_wait 30s
        max_retry_wait 1h
        disable_retry_limit true
        keepalive 300
	</store>
</match>


# These are internal fluentd log messages in case of issues,
# this is where we would see them and we can also send them
# to the aggregator or trigger alerts on them if needed.
# Example message: 2016-01-01 19:19:18 +0000 fluent.warn: {"plugin_id":"object:8ec8d0","message":"retry succeeded. plugin_id=\"object:8ec8d0\""}
<match fluent.**>
  @type stdout
</match>

#Here all messages go to die :)
<match clear>
    @type null
</match>